{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e9a83e-c199-43d0-8863-649a7be63a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6c113-cfac-4ee3-961f-ee52e80bc099",
   "metadata": {},
   "source": [
    "# Initialization settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc985e-0b82-4a16-936e-749dc23d6122",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d776d9d4-18c6-454d-b82c-8f1a29e72d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from scipy.io import savemat\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "torch.backends.cudnn.enable =True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638f38f-0f4b-4ed1-bbcb-043895b8f9e3",
   "metadata": {},
   "source": [
    "## Global HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88550241-9e00-407b-b930-8bd49185a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description = \"Global HyperParameter\")\n",
    "parser.add_argument('-f', type=str, default = \"Read additional parameters\")\n",
    "\n",
    "parser.add_argument('--Logdir', default = os.path.join('../SavedExperimentalResults'))\n",
    "parser.add_argument('--LogSavedir', default = 'Case O' )\n",
    "parser.add_argument('--savedir', default = 'Exper. 001 ( MgL )', help = \"Experimental result saving address sub-file name\")\n",
    "\n",
    "parser.add_argument('--DataRoot', default = os.path.join('/data4/dj/Datasets/Bearings/NoisyLabels'))\n",
    "parser.add_argument('--Datasets', default = [ 'CWRU', 'XJTU', 'Single', 'Damage', 'Multiple' ] )\n",
    "parser.add_argument('--NoiseType', default = { 'Asymmetric':[ 25, 30, 35, 40, 45 ], 'Symmetric':[ 50, 60, 70, 80, 90 ] } )\n",
    "\n",
    "parser.add_argument('--Dimension', default = 2**6 )\n",
    "parser.add_argument('--GBs', default = 2**7 )\n",
    "parser.add_argument('--Epochs', default = [ 5, 100 ] )\n",
    "parser.add_argument('--InitialLearningRate', default = 3.0 * 1e-3, type = float, help = 'Initial learning rate')\n",
    "parser.add_argument('--BatchSize', default = 512 )\n",
    "\n",
    "parser.add_argument('--device', default = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "parser.add_argument('--num_workers', default = 0)\n",
    "\n",
    "Args = parser.parse_args()\n",
    "if not os.path.exists(Args.Logdir):\n",
    "    os.mkdir(Args.Logdir)\n",
    "\n",
    "Args.ParentLogdir = os.path.join(Args.Logdir, Args.LogSavedir)\n",
    "if not os.path.exists(Args.ParentLogdir):\n",
    "    os.mkdir(Args.ParentLogdir)\n",
    "Args.logdir = os.path.join(Args.ParentLogdir, Args.savedir)\n",
    "if not os.path.exists(Args.logdir):\n",
    "    os.mkdir(Args.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abeec4-a3e9-408a-8b1e-64d85be6f045",
   "metadata": {},
   "source": [
    "## Definition loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8812fdb3-1381-4bc0-bbbb-09eb0d254110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTensorDataset( training, dataname, NoiseType, intensity ):\n",
    "\n",
    "    '''\n",
    "    training: 'tra', 'val' or 'tes'\n",
    "    '''\n",
    "    npy_x = training + '_samples.npy'\n",
    "    npy_y = NoiseType+  '_' + str(intensity) + '_targets.npy' if training == 'tra' else training + '_targets.npy'\n",
    "    pathes = os.path.join(Args.DataRoot, dataname)\n",
    "    \n",
    "    samples, targets = [], []\n",
    "    for domain in os.listdir(pathes):\n",
    "        \n",
    "        sample = np.load(os.path.join( os.path.join( pathes, domain ), npy_x ))\n",
    "        target = np.load(os.path.join( os.path.join( pathes, domain ), npy_y ))\n",
    "        \n",
    "        samples.append( torch.as_tensor(sample, dtype = torch.float32) )\n",
    "        targets.append( torch.as_tensor(target, dtype = torch.int64) )\n",
    "\n",
    "    dataset = TensorDataset( torch.cat(samples), torch.cat(targets) )\n",
    "    loader = DataLoader(dataset, batch_size = Args.BatchSize, num_workers = Args.num_workers, shuffle = training == 'tra' )\n",
    "    \n",
    "    return loader, target.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd71bf-43b5-407a-a027-538880be86d7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7449c0-b923-40db-86f6-5fe063a1bc12",
   "metadata": {},
   "source": [
    "@article{deng2023mgnet,\n",
    "  title={MgNet: A fault diagnosis approach for multi-bearing system based on auxiliary bearing and multi-granularity information fusion},\n",
    "  author={Deng, Jin and Liu, Han and Fang, Hairui and Shao, Siyu and Wang, Dong and Hou, Yimin and Chen, Dongsheng and Tang, Mingcong},\n",
    "  journal={Mechanical Systems and Signal Processing},\n",
    "  volume={193},\n",
    "  pages={110253},\n",
    "  year={2023},\n",
    "  publisher={Elsevier},\n",
    "  doi={10.1016/j.ymssp.2023.110253}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85b653-a941-43a7-b150-d6edda4442db",
   "metadata": {},
   "source": [
    "## basic module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbf7465-12ff-4ee6-8f92-091cf671186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, norm = None, act = None, zeros_init = False, with_bias = True ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        if zeros_init:\n",
    "            nn.init.zeros_(self.Linear.weight)\n",
    "        else:\n",
    "            nn.init.kaiming_uniform_(self.Linear.weight)\n",
    "            \n",
    "        if with_bias:\n",
    "            nn.init.zeros_(self.Linear.bias)\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.Linear(x)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.act is not None:\n",
    "            x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channel, kernel = 5, stride = 1, norm = None, act = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels, out_channel, kernel, stride, padding = int((kernel-1)/2))\n",
    "            \n",
    "        nn.init.kaiming_uniform_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.act is not None:\n",
    "            x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, number = 2):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(number, dtype=torch.float32), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w = F.relu(self.weights)\n",
    "        return x[0]*w[0] + x[1]*w[1]\n",
    "\n",
    "class ACON(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.α = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        α = F.relu(self.α)\n",
    "        x = α[0] * x * torch.sigmoid(x * α[1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b2e28-5370-4254-aac9-18bf2184e224",
   "metadata": {},
   "source": [
    "## Backbone\n",
    "\n",
    "Multi-granularity feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e30604-bacc-40fe-8fc7-e3b26b3e48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mg(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                Conv1D( d_in, d_out//4, kernel = 2**(k+2)+1, stride = 2, norm = nn.BatchNorm1d(d_out//4), act = ACON() ),  \n",
    "                Conv1D( d_out//4, d_out//4, kernel = 2**(k+2)+1, stride = 2, norm = nn.BatchNorm1d(d_out//4), act = ACON() )\n",
    "                ) for k in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.add = Add()\n",
    "        \n",
    "        self.idx = nn.Sequential( Conv1D( d_in, d_out, kernel = 1, stride = 1, norm = nn.BatchNorm1d(d_out), act = ACON() ), nn.MaxPool1d(4, 4) )\n",
    "        \n",
    "        self.point = Conv1D( d_out, d_out, kernel = 1, stride = 1, norm = nn.BatchNorm1d(d_out), act = ACON() )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        i = self.idx(x)\n",
    "        \n",
    "        xes = []\n",
    "        for conv in self.convs:\n",
    "            xes.append(conv(x))\n",
    "        x = torch.cat(xes, dim = 1)\n",
    "        \n",
    "        x = self.point(x)\n",
    "        \n",
    "        x = self.add([x, i])\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MgNet_backbone(nn.Module):\n",
    "    def __init__( self, dims, Stages = [4, 16, 64, 128], input_dim = 1 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        Stages: In the original MgNet experiment, the Stages was set to [4, 16, 64, 128] and the default single channel data input, which can be modified according to actual needs.\n",
    "        '''\n",
    "        \n",
    "        self.stages = nn.Sequential()\n",
    "        self.stages.add_module( \"Stage0\", Mg(input_dim, Stages[0]) )\n",
    "        self.stages.add_module( \"Stage1\", Mg(Stages[0], Stages[1]) )\n",
    "        self.stages.add_module( \"Stage2\", Mg(Stages[1], Stages[2]) )\n",
    "        self.stages.add_module( \"Stage3\", Mg(Stages[2], Stages[3]) )\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.as_tensor([1, 0], dtype=torch.float32), requires_grad=True)\n",
    "        self.projector = Linear( Stages[3], dims )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        weights = F.relu( self.weights )\n",
    "        z = self.stages(x)\n",
    "        \n",
    "        return self.projector( z.mean(2) * weights[0] + z.max(2)[0] * weights[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92bb15-14dc-420a-907d-bb4859e4c964",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42acedcf-99a0-4df6-a52f-bcc1690816c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_Model(nn.Module):\n",
    "    def __init__(self, categories, dims = Args.Dimension ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone =  MgNet_backbone( dims )\n",
    "        self.predictor = Linear( dims, categories, zeros_init = True )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        z = self.backbone(x)\n",
    "        prediction = self.predictor( z )\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514c2d4-1775-40b2-b5de-d42c74ab6403",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84277558-6ec7-4432-b8bf-ada8168b356b",
   "metadata": {},
   "source": [
    "## Basic training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a512c8e-dfd4-4384-993f-5c63d4d18180",
   "metadata": {},
   "source": [
    "### smooth_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a992820-4e4e-43f9-86f9-f7837e81bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_one_hot(true_labels, categories, smoothing = 0.3):\n",
    "    confidence = smoothing/(categories-1)\n",
    "    true_labels = F.one_hot(true_labels, categories)\n",
    "    with torch.no_grad():\n",
    "        smooth_label = true_labels * (1.0 - smoothing - confidence) + confidence\n",
    "    return smooth_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5671b-8f2e-436c-8cea-16e7a5422038",
   "metadata": {},
   "source": [
    "### Testing a epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e54b0771-327a-42e8-bac9-59344d75bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAccuracy( model, loader, categories ):\n",
    "    \n",
    "    model.eval().to(Args.device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(Args.device).eval()\n",
    "    \n",
    "    prefetcher = iter(loader)\n",
    "    \n",
    "    count, correct, losses = 0, 0, []\n",
    "    \n",
    "    for _ in range(len(loader)):\n",
    "        sample, target = next(prefetcher)\n",
    "        count += target.shape[0]\n",
    "        target = target.to(Args.device)\n",
    "\n",
    "        y = F.one_hot( target, num_classes=categories ).float() \n",
    "        x = sample.to(Args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(x)\n",
    "            \n",
    "        loss = criterion( prediction, y )\n",
    "        losses.append( loss.item()  )\n",
    "        correct += torch.eq(prediction.argmax(dim=1), target).sum().float().item()\n",
    "        \n",
    "    return correct/count * 100, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13cb54-b657-4320-a9af-948cea31f5e2",
   "metadata": {},
   "source": [
    "### Supervised learning for warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ac398d-d05e-4263-95c4-da3557b24419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WarmUpModel(  model, loader, optimizer, scheduler, categories  ):\n",
    "    \n",
    "    model.train()\n",
    "    model.to(Args.device)\n",
    "    criterion = nn.CrossEntropyLoss().to(Args.device).eval()\n",
    "    \n",
    "    count, correct, losses = 0, 0, []\n",
    "    scaler = GradScaler()\n",
    "    prefetcher = iter(loader)\n",
    "\n",
    "    for _ in range(len(loader)):\n",
    "        \n",
    "        sample, target = next(prefetcher)\n",
    "        count += target.shape[0]\n",
    "        \n",
    "        labels = smooth_one_hot( target.to(Args.device), categories ) \n",
    "        x = sample.to(Args.device)\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion( logits, labels )\n",
    "                \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append( loss.item()  )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        correct += torch.eq( logits.argmax(dim=1), labels.argmax(dim=1) ).sum().float().item()\n",
    "        \n",
    "    return correct/count * 100, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf1c28-f9cf-4901-8173-cb656ccb771b",
   "metadata": {},
   "source": [
    "### Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d598a111-e6b7-49e0-9af0-391c37644cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CollapseProcess(P):\n",
    "    # P is a tensor of shape (N, K) representing the probability distributions of N samples over K classes\n",
    "    # Ensure the probabilities are in the correct format (they should sum to 1 for each sample)\n",
    "    assert torch.allclose(P.sum(dim=1), torch.tensor(1.0)), \"Each probability vector should sum to 1.\"\n",
    "    \n",
    "    # Use multinomial to randomly assign labels based on the probability vectors in P\n",
    "    categories_collapsed_into = torch.multinomial(P, num_samples=1, replacement=True)\n",
    "    # Squeeze to get a 1D tensor of labels\n",
    "    categories_collapsed_into = categories_collapsed_into.squeeze()\n",
    "    \n",
    "    # collapsed_probabilities = P[torch.arange(P.size(0)), categories_collapsed_into]\n",
    "    \n",
    "    # Squeeze to get a 1D tensor of labels\n",
    "    return categories_collapsed_into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91b1c0-f88c-47c7-9a52-d40ecb41fe72",
   "metadata": {},
   "source": [
    "## Get Mg-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b564f6-0ed6-45e6-9e48-ffe34bf0982f",
   "metadata": {},
   "source": [
    "### Label correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "708bee5f-9943-4277-b131-fc5e14c3b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrectionLoader( model, loader, categories, intensity  ):\n",
    "\n",
    "    num_gb = max( 10, int(intensity * Args.GBs) )\n",
    "    model.eval()\n",
    "    model.to(Args.device)\n",
    "    \n",
    "    samples, targets, latents = [], [], []\n",
    "    for sample, target in loader:\n",
    "        x = sample.to(Args.device)\n",
    "        y = F.one_hot( target.to(Args.device), num_classes=categories ).float()  # (N, K)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = F.normalize( model.backbone( x ) )\n",
    "            \n",
    "        samples.append( x )\n",
    "        targets.append( y )\n",
    "        latents.append( z )\n",
    "        \n",
    "    samples = torch.cat(samples, 0).to('cpu')\n",
    "    targets = torch.cat(targets, 0)\n",
    "    latents = torch.cat(latents, 0)\n",
    "    \n",
    "    datasets, Intensity = [], []\n",
    "    similarity = (intensity + torch.einsum('bl, gl -> bg', latents, latents) ).exp()\n",
    "    similarity.fill_diagonal_(0)\n",
    "    \n",
    "    weight, index = torch.topk( similarity, num_gb, dim=1 )\n",
    "    superposition_state = (targets[index] * weight.unsqueeze(dim = 2)).sum(1) / weight.sum(1, keepdim = True)\n",
    "\n",
    "    purity, pseudo = superposition_state.max(dim = 1)\n",
    "    collapsed = F.one_hot( CollapseProcess( superposition_state ), num_classes=categories).float()  # (N, K)\n",
    "    pseudo = F.one_hot( pseudo, num_classes=categories).float()  # (N, K)\n",
    "\n",
    "    certainty = torch.einsum('bc, bc -> b', superposition_state, targets )\n",
    "    amplitude = torch.einsum('bc, bc -> b', superposition_state, pseudo )\n",
    "    regret = 1 - torch.einsum('bc, bc -> b', superposition_state, torch.max(pseudo, targets) )\n",
    "\n",
    "    MgLabels = torch.einsum('bc, b -> bc', targets, certainty ) + torch.einsum('bc, b -> bc', collapsed, regret ) + torch.einsum('bc, b -> bc', pseudo, amplitude )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        TensorDataset( samples, MgLabels.to('cpu') ),\n",
    "        batch_size = Args.BatchSize, num_workers = Args.num_workers,\n",
    "        pin_memory=True, shuffle = True\n",
    "    )\n",
    "    \n",
    "    return loader, 1 - purity.mean().item() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca603903-972f-4b04-b572-c8c8bea24da6",
   "metadata": {},
   "source": [
    "### Gradient update via corrected labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d821b4-04a0-4fe8-97b8-af694f5ab50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientUpdateViaCorrectedLabels( model, loader, optimizer, scheduler ):\n",
    "    \n",
    "    model.train()\n",
    "    model.to(Args.device)\n",
    "    criterion = nn.CrossEntropyLoss().to(Args.device).eval()\n",
    "    \n",
    "    count, correct, losses = 0, 0, []\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for sample, target in loader:\n",
    "        count += target.shape[0]\n",
    "        \n",
    "        y = target.to(Args.device)\n",
    "        x = sample.to(Args.device)\n",
    "        \n",
    "        with autocast():\n",
    "            \n",
    "            logits = model( x )\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append( loss.item()  )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        correct += torch.eq( logits.argmax(dim=1), y.argmax(dim=1) ).sum().float().item()\n",
    "        \n",
    "    return correct/count * 100, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401eee25-4f97-4b9e-bd9d-11a486fbc67b",
   "metadata": {},
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c6107-972b-4f75-98ff-468115c17cda",
   "metadata": {},
   "source": [
    "### Get model with trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2547fbcf-662b-4e35-95f6-e12b6c5a23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetModelWithWeights( save_root, tra_loader, val_loader, categories ):\n",
    "    \n",
    "    training_net = os.path.join( Args.logdir, 'Training net.pt' )\n",
    "    training_log = os.path.join( save_root, 'Training logs.mat' )\n",
    "    \n",
    "    Trained_net = os.path.join( save_root, 'Trained net.pt' )\n",
    "    if not os.path.exists(training_log):\n",
    "        model = Get_Model( categories )\n",
    "        model.to(Args.device)\n",
    "\n",
    "        Acc, ValAcc, Loss, ValLoss, Intensity, Max_acc, intensity = [], [], [], [], [], 0.0, 1.0\n",
    "        for e in range(len(Args.Epochs)):\n",
    "\n",
    "            epoch = Args.Epochs[e]\n",
    "            LearningRate = Args.InitialLearningRate * ( 0.8**e )\n",
    "            optimizer = torch.optim.AdamW( model.parameters(), LearningRate )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( optimizer, T_max = epoch * len(tra_loader) + 1 ) \n",
    "            \n",
    "            pbar = tqdm( range(epoch) )\n",
    "            for _ in pbar:\n",
    "                if e==0:\n",
    "                    acc, loss = WarmUpModel( model, tra_loader, optimizer, scheduler, categories )\n",
    "                else:\n",
    "                    Mg_Loader, intensity = CorrectionLoader( model, tra_loader, categories, intensity )\n",
    "                    acc, loss = GradientUpdateViaCorrectedLabels( model, Mg_Loader, optimizer, scheduler )\n",
    "                    \n",
    "                val, val_loss = GetAccuracy( model, val_loader, categories )\n",
    "                \n",
    "                if val>Max_acc:\n",
    "                    Max_acc = val\n",
    "                    torch.save(model.state_dict(), training_net)\n",
    "                    \n",
    "                pbar.set_description( r'  {} times training:        with {:.2f}% of val acc, {:.2f}% of acc, {:.3f} of loss, {:.3f} of val loss       {:.3f} of intensity.     '.format(\n",
    "                    e, val, acc,  loss, val_loss, intensity * 100\n",
    "                )\n",
    "                                     )\n",
    "                                     \n",
    "                Acc.append(acc)\n",
    "                Loss.append(loss)\n",
    "                \n",
    "                ValAcc.append(val)\n",
    "                ValLoss.append(val_loss)\n",
    "                \n",
    "                Intensity.append(intensity * 100)\n",
    "                \n",
    "            model.load_state_dict( torch.load(training_net) )\n",
    "            \n",
    "        log_results = {}\n",
    "        \n",
    "        log_results['Accuracy'] = Acc\n",
    "        log_results['ValAccuracy'] = ValAcc\n",
    "        log_results['Loss'] = Loss\n",
    "        log_results['ValLoss'] = ValLoss\n",
    "        log_results['Intensity'] = Intensity\n",
    "        \n",
    "        savemat( training_log, log_results )\n",
    "        torch.save(model.state_dict(), Trained_net)\n",
    "        \n",
    "    else:\n",
    "        model = Get_Model( categories )\n",
    "        model.load_state_dict( torch.load(Trained_net) )\n",
    "        model.to(Args.device)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905aca4-9f2a-46cc-971b-a1f188d839f0",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2899472e-fc7a-43c0-a0be-8dad63b08532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training( training_times = 10 ):\n",
    "    \n",
    "    test_results = os.path.join( Args.logdir, 'Test results.txt')\n",
    "    \n",
    "    if not os.path.exists(test_results):\n",
    "        Header = '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\n'.format( 'Dataset', 'Noise type', 'Noise intensity', 'Times', 'Accuracy(%)', 'Loss' )\n",
    "        with open(test_results, 'a+') as log:\n",
    "            log.write(Header)\n",
    "\n",
    "    for dataname in Args.Datasets:\n",
    "        print('-----------------------------------------------------------', dataname, '-----------------------------------------------------------')\n",
    "        for nt in Args.NoiseType.keys():\n",
    "            print()\n",
    "            print('====================================================================')\n",
    "            for intensity in Args.NoiseType[nt]:\n",
    "                print(nt, ' noisy        ', intensity, '%')\n",
    "                \n",
    "                for times in range(training_times):\n",
    "                    times += 1\n",
    "                    \n",
    "                    f = open(test_results, encoding='gbk')\n",
    "                    Results=[]\n",
    "                    for line in f:\n",
    "                        Results.append(line.strip().split('\\t'))\n",
    "                        \n",
    "                    IfResult = [\n",
    "                        result for result in Results \n",
    "                        if result[0] == dataname\n",
    "                        and result[1] == nt\n",
    "                        and result[2] == str(intensity)\n",
    "                        and result[3] == str(times)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(IfResult) > 0:\n",
    "                        for results in IfResult:\n",
    "                            print(\n",
    "                                r'      On {} dataset with {} noise of {}% {}-times:        accuracy of {:.3f}% with loss of {:.4f}.'.format(\n",
    "                                    results[0], results[1], results[2], results[3], float(results[4]), float(results[5])\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        save_root = os.path.join( Args.logdir, dataname ) \n",
    "                        if not os.path.exists(save_root):\n",
    "                            os.mkdir(save_root)\n",
    "                            \n",
    "                        save_root = os.path.join( save_root, nt ) \n",
    "                        if not os.path.exists(save_root):\n",
    "                            os.mkdir(save_root)\n",
    "                        save_root = os.path.join( save_root, str(intensity) ) \n",
    "                        if not os.path.exists(save_root):\n",
    "                            os.mkdir(save_root)\n",
    "                            \n",
    "                        save_root = os.path.join( save_root, str(times) + ' times' ) \n",
    "                        if not os.path.exists(save_root):\n",
    "                            os.mkdir(save_root)\n",
    "                            \n",
    "                        tes_loader, categories = GetTensorDataset( 'tes', dataname, nt, intensity )\n",
    "                        \n",
    "                        tra_loader, categories = GetTensorDataset( 'tra', dataname, nt, intensity )\n",
    "                        val_loader, categories = GetTensorDataset( 'val', dataname, nt, intensity )\n",
    "                        \n",
    "                        model = GetModelWithWeights( save_root, tra_loader, val_loader, categories )\n",
    "                        acc, loss = GetAccuracy( model, tes_loader, categories )\n",
    "                        \n",
    "                        results = '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\n'.format( dataname, nt, intensity, times, acc, loss )\n",
    "                        with open(test_results, 'a+') as log:\n",
    "                            log.write(results)\n",
    "                            \n",
    "                        results = results.split('\\t')\n",
    "                        print(\n",
    "                            r'      On {} dataset with {} noise of {}% {}-times:        accuracy of {:.3f}% with loss of {:.4f}.'.format(\n",
    "                                results[0], results[1], results[2], results[3], float(results[4]), float(results[5]) )\n",
    "                            )\n",
    "                        \n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf2902-9f44-4e0d-9ec1-39f7e65c835a",
   "metadata": {},
   "source": [
    "# Experimental results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a904c0-4a7e-4851-8143-06ae72c6e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------- CWRU -----------------------------------------------------------\n",
      "\n",
      "====================================================================\n",
      "Asymmetric  noisy         25 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32c123ead884c4fa521379caa873153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75d96864430486fb8a909d212b54285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Training( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254dd42-0b6c-4af4-b58c-008b68d3d458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
